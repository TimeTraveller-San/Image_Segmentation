{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import albumentations as albu\n",
    "import configparser\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "# Catalyst is amazing.\n",
    "from catalyst.data import Augmentor\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
    "# from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.dl.runner import SupervisedWandbRunner as SupervisedRunner\n",
    "from catalyst.contrib.models.segmentation import Unet\n",
    "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
    "\n",
    "# PyTorch made my work much much easier.\n",
    "import segmentation_models_pytorch as smp\n",
    "from dataloader import SegmentationDataset, SegmentationDatasetTest\n",
    "from augmentations import get_training_augmentation, get_validation_augmentation, get_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')\n",
    "\n",
    "def get_ids():\n",
    "    train = pd.read_csv(f'../input/train.csv')\n",
    "    train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "    train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "    id_mask_count = train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().\\\n",
    "    reset_index().rename(columns={'index': 'img_id', 'Image_Label': 'count'})\n",
    "    train_ids, valid_ids = train_test_split(id_mask_count['img_id'].values, random_state=42, stratify=id_mask_count['count'], test_size=0.1)\n",
    "    return train_ids, valid_ids\n",
    "\n",
    "def get_loaders(bs=32, num_workers=4, preprocessing_fn=None):\n",
    "        train_dataset = SegmentationDataset(ids=train_ids,\n",
    "                    transforms=get_training_augmentation(),\n",
    "                    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                    img_db=\"../input/train_images_525/train_images_525\",\n",
    "                    mask_db=\"../input/mask\")\n",
    "        \n",
    "        valid_dataset = SegmentationDataset(ids=valid_ids,\n",
    "                    transforms=get_validation_augmentation(),\n",
    "                    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                    img_db=\"../input/train_images_525/train_images_525\",\n",
    "                    mask_db=\"../input/mask\")\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs,\n",
    "            shuffle=True, num_workers=num_workers)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=bs,\n",
    "            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        loaders = {\n",
    "            \"train\": train_loader,\n",
    "            \"valid\": valid_loader\n",
    "        }\n",
    "        return loaders\n",
    "\n",
    "def get_model(encoder='resnet18', classes=4):\n",
    "    encoder_weights = 'imagenet'\n",
    "    model = smp.Unet(\n",
    "        encoder_name=encoder,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=classes,\n",
    "        activation=None,\n",
    "    )\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, encoder_weights)\n",
    "    return model, preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timetraveller/.local/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2567: UserWarning:\n",
      "\n",
      "Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../configs/config.ini')\n",
    "conf = config['DEFAULT']\n",
    "\n",
    "lrd = conf.getfloat('lrd')\n",
    "lre = conf.getfloat('lre')\n",
    "epochs = conf.getint('epochs')\n",
    "num_workers = conf.getint('num_workers')\n",
    "encoder = conf.get('encoder')\n",
    "logdir = conf.get('logdir')\n",
    "bs = 2\n",
    "train_ids, valid_ids = get_ids()\n",
    "model, preprocessing_fn = get_model(encoder)\n",
    "loaders = get_loaders(bs, num_workers, preprocessing_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "        {'params': model.decoder.parameters(), 'lr': lrd},\n",
    "        {'params': model.encoder.parameters(), 'lr': lre},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pr, gt, eps=1e-9, threshold=None, activation='sigmoid'):\n",
    "    if activation is None or activation == \"none\":\n",
    "        activation_fn = lambda x: x\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_fn = torch.nn.Sigmoid()\n",
    "    elif activation == \"softmax2d\":\n",
    "        activation_fn = torch.nn.Softmax2d()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Activation implemented for sigmoid and softmax2d\"\n",
    "        )\n",
    "\n",
    "    pr = activation_fn(pr)\n",
    "\n",
    "    if threshold is not None:\n",
    "        pr = (pr > threshold).float()\n",
    "\n",
    "\n",
    "    intersection = torch.sum(gt * pr)\n",
    "    sum1 = torch.sum(pr)\n",
    "    sum2 = torch.sum(gt)\n",
    "\n",
    "    score = (2.0 * intersection + eps) / (sum1 + sum2 + eps)\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    __name__ = 'dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-9, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        return 1 - dice(y_pr, y_gt, eps=self.eps,\n",
    "                    threshold=None, activation=self.activation)\n",
    "\n",
    "\n",
    "class BCEDiceLoss(DiceLoss):\n",
    "    __name__ = 'bce_dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-9, activation='sigmoid'):\n",
    "        super().__init__(eps, activation)\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, y_pr, y_gt):\n",
    "        dice = super().forward(y_pr, y_gt)\n",
    "        bce = self.bce(y_pr, y_gt)\n",
    "        return 0.5*dice + 0.5*bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor(0.7413, grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 320, 640])\n",
      "torch.Size([2, 3, 320, 640])\n",
      "torch.Size([2, 4, 320, 640])\n"
     ]
    }
   ],
   "source": [
    "criteria = BCEDiceLoss()\n",
    "\n",
    "for batch in loaders['train']:\n",
    "    print(len(batch))\n",
    "    img, mask = batch\n",
    "    output = model(img)\n",
    "    loss = criteria(output, mask)\n",
    "    print(loss)\n",
    "    print(output.shape)\n",
    "    print(img.shape)\n",
    "    print(mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2264, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "eps = 1\n",
    "gt, pr = mask, output\n",
    "activation_fn = torch.nn.Sigmoid()\n",
    "pr = activation_fn(pr)\n",
    "intersection = torch.sum(gt * pr)\n",
    "sum1 = torch.sum(pr)\n",
    "sum2 = torch.sum(gt)\n",
    "score = (2.0 * intersection + eps) / (sum1 + sum2 + eps)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1877, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-7\n",
    "gt, pr = mask, output\n",
    "activation_fn = torch.nn.Sigmoid()\n",
    "pr = activation_fn(pr)\n",
    "intersection = torch.sum(gt * pr, axis=[0, 2, 3])\n",
    "sum1 = torch.sum(pr, axis=[0, 2, 3])\n",
    "sum2 = torch.sum(gt, axis=[0, 2, 3])\n",
    "score = (2.0 * intersection + eps) / (sum1 + sum2 + eps)\n",
    "print(torch.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
