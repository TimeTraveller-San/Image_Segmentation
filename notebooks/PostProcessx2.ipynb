{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import albumentations as albu\n",
    "import configparser\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "# Catalyst is amazing.\n",
    "from catalyst.data import Augmentor\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "# from catalyst.dl.runner import SupervisedWandbRunner as SupervisedRunner\n",
    "from catalyst.contrib.models.segmentation import Unet\n",
    "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
    "\n",
    "# PyTorch made my work much much easier.\n",
    "import segmentation_models_pytorch as smp\n",
    "from dataloader import SegmentationDataset, SegmentationDatasetTest, SegmentationDataset_withid\n",
    "from augmentations import get_training_augmentation, get_preprocessing\n",
    "from augmentations import get_test_augmentation, get_validation_augmentation\n",
    "\n",
    "from utils import *\n",
    "from metric import dice\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on unet architecture with efficientnet-b4 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2567: UserWarning:\n",
      "\n",
      "Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ids(train_ids_file='../train_ids.pkl', valid_ids_file='../valid_ids.pkl'):\n",
    "    with open(train_ids_file, 'rb') as handle:\n",
    "        train_ids = pickle.load(handle)\n",
    "\n",
    "    with open(valid_ids_file, 'rb') as handle:\n",
    "        valid_ids = pickle.load(handle)\n",
    "\n",
    "    return train_ids, valid_ids\n",
    "\n",
    "train_ids, valid_ids = get_ids()\n",
    "# valid_ids = list(train_ids)+list(valid_ids)\n",
    "\n",
    "\n",
    "# FIX LOADERS\n",
    "\n",
    "def get_loaders(bs=32, num_workers=4, preprocessing_fn=None,\n",
    "            img_db=\"../input/train_images_480/\", mask_db=\"../input/train_masks_480/\",\n",
    "            npy=True):\n",
    "        train_ids, valid_ids = get_ids()\n",
    "\n",
    "        train_dataset = SegmentationDataset(ids=train_ids,\n",
    "                    transforms=get_training_augmentation(),\n",
    "                    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                    img_db=img_db,\n",
    "                    mask_db=mask_db, npy=npy)\n",
    "        valid_dataset = SegmentationDataset(ids=valid_ids,\n",
    "                    transforms=get_validation_augmentation(),\n",
    "                    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                    img_db=img_db,\n",
    "                    mask_db=mask_db, npy=npy)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=bs,\n",
    "            shuffle=True, num_workers=num_workers)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=bs,\n",
    "            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        loaders = {\n",
    "            \"train\": train_loader,\n",
    "            \"valid\": valid_loader\n",
    "        }\n",
    "        return valid_dataset, loaders\n",
    "    \n",
    "bs = 8    \n",
    "num_workers = 0\n",
    "encoder = 'efficientnet-b4'\n",
    "arch = 'unet'\n",
    "model, preprocessing_fn = get_model(encoder, type=arch)\n",
    "valid_dataset, loaders = get_loaders(bs, num_workers, preprocessing_fn)\n",
    "train_loader = loaders['train']\n",
    "valid_loader = loaders['valid']\n",
    "\n",
    "# model, preprocessing_fn = get_model(encoder)\n",
    "# loaders = get_loaders(bs, num_workers, preprocessing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "=> loading checkpoint ../logs/unet_efficientnet-b4/checkpoints/best.pth\n",
      "loaded checkpoint ../logs/unet_efficientnet-b4/checkpoints/best.pth (epoch 17)\n",
      "Top best models:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model\")\n",
    "model_path = f\"../logs/unet_efficientnet-b4/checkpoints/best.pth\"\n",
    "\n",
    "runner = SupervisedRunner()\n",
    "encoded_pixels = []\n",
    "loaders = {\"infer\": valid_loader}\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=model_path),\n",
    "        InferCallback()\n",
    "    ],\n",
    ")\n",
    "loaders['train'] = train_loader\n",
    "loaders['valid'] = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [02:46,  4.34it/s]"
     ]
    }
   ],
   "source": [
    "valid_masks = []\n",
    "LIMIT = 800\n",
    "probabilities = np.zeros((int(LIMIT*4), 320, 480)) #HARDCODED FOR NOW\n",
    "for i, (batch, output) in enumerate(tqdm.tqdm(zip(valid_dataset, runner.callbacks[0].predictions[\"logits\"]))):\n",
    "        if i >= LIMIT:\n",
    "            break\n",
    "        image, mask = batch\n",
    "        for m in mask:\n",
    "            # if m.shape != (350, 525):\n",
    "            #     m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "            valid_masks.append(m)\n",
    "\n",
    "        for j, probability in enumerate(output):\n",
    "            # if probability.shape != (350, 525):\n",
    "            #     probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "            probabilities[i * 4 + j, :, :] = probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(probability, threshold, min_size, \n",
    "                 threshold_type='percentile', size=(350, 525)):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    if threshold_type == 'mean':\n",
    "        threshold = np.mean(probability)\n",
    "    elif threshold_type == 'percentile':    \n",
    "        threshold = np.percentile(probability, threshold)    \n",
    "        \n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros(size, np.float32)\n",
    "    num = 0\n",
    "    \n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num\n",
    "\n",
    "def does_not_have(img_name, class_id, df):\n",
    "    if class_id in df[df.img_name==img_name].label.values:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "MAPPING = dict(zip(CLASSES, [x for x in range(4)]))\n",
    "_, valid_ids = get_ids()\n",
    "valid_ids = valid_ids[:LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img_name  label\n",
       "0  0011165.jpg      0\n",
       "1  0011165.jpg      1\n",
       "4  002be4f.jpg      0\n",
       "5  002be4f.jpg      1\n",
       "7  002be4f.jpg      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/train.csv')\n",
    "df = df[~df.EncodedPixels.isna()]\n",
    "df['img_name'] = df.Image_Label.apply(lambda x: x.split('_')[0])\n",
    "df['label'] = df.Image_Label.apply(lambda x: MAPPING[x.split('_')[1]])\n",
    "df.drop(['Image_Label', 'EncodedPixels'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:19<06:03, 19.12s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:38<05:43, 19.06s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:57<05:28, 19.31s/it]\u001b[A\n",
      " 20%|██        | 4/20 [01:18<05:16, 19.76s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [01:40<05:03, 20.25s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:01<04:49, 20.69s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:23<04:32, 20.95s/it]\u001b[A\n",
      " 40%|████      | 8/20 [02:44<04:13, 21.10s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [03:06<03:53, 21.25s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [03:27<03:31, 21.12s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [03:48<03:09, 21.02s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [04:08<02:47, 20.88s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [04:29<02:25, 20.73s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [04:49<02:03, 20.56s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [05:09<01:42, 20.50s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [05:29<01:20, 20.22s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [05:48<00:59, 19.97s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [06:07<00:39, 19.70s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [06:26<00:19, 19.42s/it]\u001b[A\n",
      "100%|██████████| 20/20 [06:45<00:00, 20.25s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold   size      dice\n",
      "28         20  25000  0.720396\n",
      "33         25  20000  0.720247\n",
      "29         20  27000  0.720214\n",
      "27         20  20000  0.719970\n",
      "32         25  15000  0.719828\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:18<05:44, 18.16s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:35<05:24, 18.04s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:54<05:07, 18.10s/it]\u001b[A\n",
      " 20%|██        | 4/20 [01:13<04:54, 18.39s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [01:32<04:41, 18.78s/it]\u001b[A\n",
      " 30%|███       | 6/20 [01:52<04:26, 19.06s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:12<04:11, 19.32s/it]\u001b[A\n",
      " 40%|████      | 8/20 [02:32<03:55, 19.60s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [02:52<03:35, 19.57s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [03:11<03:15, 19.55s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [03:31<02:55, 19.47s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [03:50<02:35, 19.40s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [04:09<02:14, 19.27s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [04:28<01:54, 19.16s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [04:46<01:34, 18.98s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [05:05<01:15, 18.82s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [05:23<00:55, 18.59s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [05:41<00:36, 18.38s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [05:58<00:18, 18.14s/it]\u001b[A\n",
      "100%|██████████| 20/20 [06:16<00:00, 18.82s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold   size      dice\n",
      "23         15  27000  0.769522\n",
      "22         15  25000  0.769481\n",
      "16         10  25000  0.769471\n",
      "17         10  27000  0.769471\n",
      "13         10  10000  0.769459\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:19<06:04, 19.18s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:38<05:44, 19.13s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:57<05:27, 19.29s/it]\u001b[A\n",
      " 20%|██        | 4/20 [01:18<05:16, 19.78s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [01:40<05:06, 20.41s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:03<04:55, 21.11s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:26<04:43, 21.80s/it]\u001b[A\n",
      " 40%|████      | 8/20 [02:50<04:27, 22.27s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [03:13<04:08, 22.59s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [03:36<03:47, 22.77s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [03:59<03:25, 22.80s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [04:22<03:03, 22.88s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [04:45<02:39, 22.74s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [05:07<02:15, 22.59s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [05:28<01:51, 22.25s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [05:49<01:27, 21.89s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [06:10<01:04, 21.56s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [06:30<00:42, 21.09s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [06:50<00:20, 20.63s/it]\u001b[A\n",
      "100%|██████████| 20/20 [07:09<00:00, 21.46s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold   size      dice\n",
      "23         15  27000  0.701414\n",
      "29         20  27000  0.700510\n",
      "22         15  25000  0.700453\n",
      "21         15  20000  0.700386\n",
      "27         20  20000  0.700313\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [00:21<06:40, 21.10s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:42<06:21, 21.17s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [01:04<06:06, 21.56s/it]\u001b[A\n",
      " 20%|██        | 4/20 [01:27<05:52, 22.01s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [01:52<05:39, 22.63s/it]\u001b[A\n",
      " 30%|███       | 6/20 [02:16<05:24, 23.21s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [02:41<05:07, 23.64s/it]\u001b[A\n",
      " 40%|████      | 8/20 [03:05<04:47, 23.95s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [03:30<04:24, 24.06s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [03:54<04:00, 24.01s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [04:17<03:35, 23.91s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [04:40<03:09, 23.66s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [05:03<02:43, 23.40s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [05:26<02:19, 23.23s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [05:48<01:54, 22.96s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [06:10<01:30, 22.61s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [06:32<01:06, 22.31s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [06:53<00:43, 21.94s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [07:14<00:21, 21.77s/it]\u001b[A\n",
      "100%|██████████| 20/20 [07:35<00:00, 22.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    threshold   size      dice\n",
      "27         20  20000  0.575085\n",
      "29         20  27000  0.574940\n",
      "23         15  27000  0.574555\n",
      "31         25  10000  0.574517\n",
      "26         20  15000  0.574498\n",
      "{0: (20, 25000), 1: (15, 27000), 2: (15, 27000), 3: (20, 20000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "size = (320, 480)\n",
    "class_params = {}\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "for class_id in range(4):\n",
    "        print(class_id)\n",
    "        attempts = []\n",
    "        for t in tqdm.tqdm(range(0, 100, 5)):\n",
    "#             t /= 100\n",
    "            for ms in [5000, 10000, 15000, 20000, 25000, 27000]:\n",
    "                masks = []\n",
    "                for i, img_name in zip(range(class_id, len(probabilities), 4), valid_ids):\n",
    "                    if does_not_have(img_name, class_id, df):\n",
    "                        predict = np.zeros(size)\n",
    "                    else:    \n",
    "                        probability = probabilities[i]\n",
    "                        predict, num_predict = post_process(sigmoid(probability), t,\n",
    "                                                            ms, size=size)\n",
    "                    masks.append(predict)\n",
    "                d = []\n",
    "                for i, j in zip(masks, valid_masks[class_id::4]):\n",
    "                    if (i.sum() == 0) & (j.sum() == 0):\n",
    "                        d.append(1)\n",
    "                    else:\n",
    "                        d.append(dice(i, j))\n",
    "                attempts.append((t, ms, np.mean(d)))\n",
    "\n",
    "        attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n",
    "        attempts_df = attempts_df.sort_values('dice', ascending=False)\n",
    "        print(attempts_df.head())\n",
    "        best_threshold = attempts_df['threshold'].values[0]\n",
    "        best_size = attempts_df['size'].values[0]\n",
    "\n",
    "        class_params[class_id] = (best_threshold, best_size)\n",
    "print(class_params)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on unet architecture with efficientnet-b4 encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2567: UserWarning:\n",
      "\n",
      "Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint ../logs/unet_efficientnet-b4/checkpoints/best.pth\n",
      "loaded checkpoint ../logs/unet_efficientnet-b4/checkpoints/best.pth (epoch 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/925 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top best models:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 925/925 [02:57<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3301 masks would be removed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import albumentations as albu\n",
    "import configparser\n",
    "import argparse\n",
    "import wandb\n",
    "\n",
    "# Catalyst is amazing.\n",
    "from catalyst.data import Augmentor\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "# from catalyst.dl.runner import SupervisedWandbRunner as SupervisedRunner\n",
    "from catalyst.contrib.models.segmentation import Unet\n",
    "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
    "\n",
    "# PyTorch made my work much much easier.\n",
    "import segmentation_models_pytorch as smp\n",
    "from dataloader import SegmentationDataset, SegmentationDatasetTest, SegmentationDataset_withid\n",
    "from augmentations import get_training_augmentation, get_preprocessing\n",
    "from augmentations import get_test_augmentation, get_validation_augmentation\n",
    "\n",
    "from utils import *\n",
    "from metric import dice\n",
    "import pickle\n",
    "\n",
    "class_params = {0: (20, 25000), 1: (15, 27000), 2: (15, 27000), 3: (20, 20000)}\n",
    "\n",
    "def post_process(probability, threshold, min_size, \n",
    "                 threshold_type='percentile', size=(350, 525)):\n",
    "    \"\"\"\n",
    "    Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored\n",
    "    \"\"\"\n",
    "    # don't remember where I saw it\n",
    "    if threshold_type == 'mean':\n",
    "        threshold = np.mean(probability)\n",
    "    elif threshold_type == 'percentile':    \n",
    "        threshold = np.percentile(probability, threshold)    \n",
    "        \n",
    "    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros(size, np.float32)\n",
    "    num = 0\n",
    "    \n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num\n",
    "\n",
    "\n",
    "def get_ids(train_ids_file='../train_ids.pkl', valid_ids_file='../valid_ids.pkl'):\n",
    "    with open(train_ids_file, 'rb') as handle:\n",
    "        train_ids = pickle.load(handle)\n",
    "\n",
    "    with open(valid_ids_file, 'rb') as handle:\n",
    "        valid_ids = pickle.load(handle)\n",
    "\n",
    "    return train_ids, valid_ids\n",
    "\n",
    "def get_loaders(bs=32, num_workers=4, preprocessing_fn=None,\n",
    "            img_db=\"../input/train_images_480/\", mask_db=\"../input/train_masks_480/\",\n",
    "            npy=True):\n",
    "        train_ids, valid_ids = get_ids()\n",
    "\n",
    "        valid_ids = valid_ids[:100]\n",
    "\n",
    "        valid_dataset = SegmentationDataset(ids=valid_ids,\n",
    "                    transforms=get_validation_augmentation(),\n",
    "                    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                    img_db=img_db,\n",
    "                    mask_db=mask_db, npy=npy)\n",
    "\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=bs,\n",
    "            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        loaders = {\n",
    "            \"infer\": valid_loader\n",
    "        }\n",
    "        return loaders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "bs = 4\n",
    "num_workers = 0\n",
    "encoder = 'efficientnet-b4'\n",
    "arch = 'unet'\n",
    "model, preprocessing_fn = get_model(encoder, type=arch)\n",
    "model_path = f\"../logs/unet_efficientnet-b4/checkpoints/best.pth\"\n",
    "\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "sub = pd.read_csv(f'../input/sample_submission.csv')\n",
    "sub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "sub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "test_ids = sub['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values\n",
    "\n",
    "# Load model, weird way in catalyst\n",
    "loaders = get_loaders()\n",
    "# checkpoint = torch.load(model_path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "runner = SupervisedRunner()\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=model_path),\n",
    "        InferCallback()\n",
    "    ],\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDatasetTest(test_ids,\n",
    "                                        transforms=get_test_augmentation(),\n",
    "                                        preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                                        img_db=\"../input/test_images_525/test_images_525\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False,\n",
    "                            num_workers=num_workers)\n",
    "\n",
    "loaders = {\"test\": test_loader}\n",
    "\n",
    "# {0: (20, 25000), 1: (15, 27000), 2: (15, 27000), 3: (20, 20000)}\n",
    "\n",
    "encoded_pixels = []\n",
    "image_id = 0\n",
    "size = (350, 525) #Required output size by kaggle\n",
    "for i, test_batch in enumerate(tqdm.tqdm(loaders['test'])):\n",
    "    runner_out = runner.predict_batch({\"features\": test_batch.cuda()})['logits']\n",
    "    for i, batch in enumerate(runner_out):\n",
    "        for probability in batch:\n",
    "            probability = probability.cpu().detach().numpy()\n",
    "            if probability.shape != (350, 525):\n",
    "                probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "            predict, num_predict = post_process(sigmoid(probability),\n",
    "                                                class_params[image_id % 4][0],\n",
    "                                                class_params[image_id % 4][1],\n",
    "                                                size=size)\n",
    "            if num_predict == 0:\n",
    "                encoded_pixels.append('')\n",
    "            else:\n",
    "                r = mask2rle(predict)\n",
    "                encoded_pixels.append(r)\n",
    "            image_id += 1\n",
    "\n",
    "sub['EncodedPixels'] = encoded_pixels\n",
    "\n",
    "# Use classifer\n",
    "import pickle\n",
    "with open('../list.pkl', 'rb') as handle:\n",
    "    image_labels_empty = pickle.load(handle)\n",
    "\n",
    "predictions_nonempty = set(sub.loc[~sub['EncodedPixels'].isnull(), 'Image_Label'].values)\n",
    "print(f'{len(image_labels_empty.intersection(predictions_nonempty))} masks would be removed')\n",
    "\n",
    "sub.loc[sub['Image_Label'].isin(image_labels_empty), 'EncodedPixels'] = np.nan\n",
    "sub.to_csv(\"UnetEffnet-b4WithMyPP.csv\", columns=['Image_Label', 'EncodedPixels'], index=False)\n",
    "\n",
    "\n",
    "# git fetch --all && git reset --hard origin/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>label</th>\n",
       "      <th>im_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>002f507.jpg_Fish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fish</td>\n",
       "      <td>002f507.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>002f507.jpg_Flower</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flower</td>\n",
       "      <td>002f507.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>002f507.jpg_Gravel</td>\n",
       "      <td>3 344 352 346 701 349 1051 65099 66151 349 665...</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>002f507.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>002f507.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>002f507.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0035ae9.jpg_Fish</td>\n",
       "      <td>71 143 432 46 504 16 776 135 1125 138 1475 139...</td>\n",
       "      <td>Fish</td>\n",
       "      <td>0035ae9.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    002f507.jpg_Fish                                                NaN   \n",
       "1  002f507.jpg_Flower                                                NaN   \n",
       "2  002f507.jpg_Gravel  3 344 352 346 701 349 1051 65099 66151 349 665...   \n",
       "3   002f507.jpg_Sugar                                                NaN   \n",
       "4    0035ae9.jpg_Fish  71 143 432 46 504 16 776 135 1125 138 1475 139...   \n",
       "\n",
       "    label        im_id  \n",
       "0    Fish  002f507.jpg  \n",
       "1  Flower  002f507.jpg  \n",
       "2  Gravel  002f507.jpg  \n",
       "3   Sugar  002f507.jpg  \n",
       "4    Fish  0035ae9.jpg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
